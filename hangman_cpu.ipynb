{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPpVsRCoYlTc",
        "outputId": "02e2dbeb-2339-4864-ba82-44aad38e911f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_ai_player.py  sample_data  words.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Junfeng Li\n",
        "\n",
        "Project Overview (Non-Parallel Model):\n",
        "1. This implementation trains a deep learning model for the Hangman game using PyTorch on Google Colab.\n",
        "2. Training data is generated dynamically by simulating the game process, encoding word states and guessed letters.\n",
        "3. The model utilizes an LSTM architecture to predict the next letter during the game.\n",
        "\n",
        "Key Learnings and Thought Process:\n",
        "- Data Generation: Experimented with methods to generate balanced training samples, ensuring a diverse dataset for effective learning.\n",
        "- Training Pipeline: Gained experience in setting up PyTorch pipelines, including tensor creation, DataLoader usage, and batch processing.\n",
        "- Model Design: Used LSTM to capture the sequence nature of the game, learning the relationship between word states and guessed letters.\n",
        "\n",
        "Challenges and Future Improvements:\n",
        "1. The data generation process is time-intensive for larger datasets, motivating the need for optimization.\n",
        "2. Further work could involve exploring alternative architectures or hyperparameter tuning to improve accuracy and efficiency.\n",
        "\n",
        "Some refrence：\n",
        "1.https://docs.python.org/3/library/multiprocessing.html\n",
        "2.https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html\n",
        "3.https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html\n",
        "4.https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "5.https://www.geeksforgeeks.org/loading-data-in-pytorch/\n",
        "6.https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "\n",
        "Note: Luke managed the local testing and might modify something based on his own preference\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from new_ai_player import HangmanLSTM\n",
        "\n",
        "\n",
        "def generate_training_data(word_list, num_samples, max_word_length=10):\n",
        "    \"\"\"\n",
        "    Generate training data based on a list of words.\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of words to create training examples.\n",
        "        num_samples (int): The number of training examples to generate.\n",
        "        max_word_length (int): The maximum length of the words to handle.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of training examples, each containing the current word state,\n",
        "              guessed letters, and the target letter.\n",
        "    \"\"\"\n",
        "    print(\"Generating training data...\")\n",
        "    data = []  # This will hold all the training examples\n",
        "\n",
        "    for _ in range(num_samples):  # Repeat for the required number of examples\n",
        "        word = random.choice(word_list)  # Pick a random word from the list\n",
        "        word_letters = set(word)  # Find all unique letters in the word\n",
        "        guessed_letters = set()  # Start with no letters guessed\n",
        "        obscured_word = ['_' for _ in word]  # Show the word as blank initially\n",
        "\n",
        "        # Keep guessing letters until we've guessed every unique letter in the word\n",
        "        while len(guessed_letters) < len(word_letters):\n",
        "            next_letter = random.choice(list(word_letters - guessed_letters))  # Pick an unguessed letter\n",
        "            guessed_letters.add(next_letter)  # Add the new letter to guessed letters\n",
        "\n",
        "            # Update the blanks to show the guessed letters\n",
        "            for i, char in enumerate(word):\n",
        "                if char in guessed_letters:\n",
        "                    obscured_word[i] = char\n",
        "\n",
        "            # Convert the current state of the word into numbers\n",
        "            word_input = encode_word_state(''.join(obscured_word), max_word_length)\n",
        "            # Convert the guessed letters into numbers\n",
        "            guessed_input = encode_guessed_letters(guessed_letters)\n",
        "            # Convert the guessed letter to its position in the alphabet (0 for 'a', 25 for 'z')\n",
        "            target_letter = ord(next_letter) - ord('a')\n",
        "\n",
        "            # If the word is still incomplete, add the sample multiple times to balance the data\n",
        "            if '_' in ''.join(obscured_word):\n",
        "                for _ in range(3):  # Add the same sample 3 times\n",
        "                    data.append((word_input, guessed_input, target_letter))\n",
        "            else:\n",
        "                data.append((word_input, guessed_input, target_letter))  # Add once if the word is complete\n",
        "\n",
        "    print(f\"Generated {len(data)} training samples.\")  # Print the total number of samples\n",
        "    return data  # Return the training examples\n",
        "\n",
        "\n",
        "def encode_word_state(word_display, max_word_length):\n",
        "    \"\"\"\n",
        "    Turn the current word display into a matrix of numbers.\n",
        "\n",
        "    Args:\n",
        "        word_display (str): What the word looks like now (e.g., \"_ppl_\" for \"apple\").\n",
        "        max_word_length (int): How many characters to process.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A matrix where each row represents one letter as numbers.\n",
        "    \"\"\"\n",
        "    word_vector = np.zeros((max_word_length, 27))  # Start with all zeros\n",
        "    for i, char in enumerate(word_display[:max_word_length]):  # Go through each character in the word\n",
        "        if char == '_':\n",
        "            word_vector[i, 26] = 1  # The last column (27th) is for blanks\n",
        "        elif 'a' <= char <= 'z':\n",
        "            word_vector[i, ord(char) - ord('a')] = 1  # Put a 1 in the column for the correct letter\n",
        "    return word_vector  # Give back the matrix\n",
        "\n",
        "\n",
        "def encode_guessed_letters(guessed_letters):\n",
        "    \"\"\"\n",
        "    Turn the guessed letters into a one-hot vector.\n",
        "\n",
        "    Args:\n",
        "        guessed_letters (set): Letters the player has guessed so far.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A vector with a 1 for each guessed letter.\n",
        "    \"\"\"\n",
        "    guessed_vector = np.zeros(26)  # Start with a vector of 26 zeros\n",
        "    for letter in guessed_letters:  # Go through each guessed letter\n",
        "        guessed_vector[ord(letter) - ord('a')] = 1  # Set the correct position to 1\n",
        "    return guessed_vector  # Give back the vector\n",
        "\n",
        "\n",
        "def train_model(word_list, model_path='large_hangman_model_cpu_non_parallel.pth', num_samples=10000, epochs=25, batch_size=32, lr=0.001):\n",
        "    \"\"\"\n",
        "    Train the HangmanLSTM model.\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of words for training.\n",
        "        model_path (str): The path to save the trained model.\n",
        "        num_samples (int): Number of training examples to generate.\n",
        "        epochs (int): Number of iterations over the entire dataset.\n",
        "        batch_size (int): Number of examples per training batch.\n",
        "        lr (float): Learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Generate training data\n",
        "    print(\"Generating training data...\")\n",
        "    start_time_data = time.time()\n",
        "    data = generate_training_data(word_list, num_samples)  # Create training examples\n",
        "    end_time_data = time.time()\n",
        "    print(f\"Data generation took {end_time_data - start_time_data:.2f} seconds.\")\n",
        "\n",
        "    # Prepare tensors\n",
        "    print(\"Preparing data tensors...\")\n",
        "    inputs_word = torch.tensor(np.array([item[0] for item in data]), dtype=torch.float32)  # Word inputs\n",
        "    inputs_guessed = torch.tensor(np.array([item[1] for item in data]), dtype=torch.float32)  # Guessed letters\n",
        "    targets = torch.tensor(np.array([item[2] for item in data]), dtype=torch.long)  # Target letters\n",
        "\n",
        "    # Create DataLoader\n",
        "    print(\"Creating DataLoader...\")\n",
        "    dataset = torch.utils.data.TensorDataset(inputs_word, inputs_guessed, targets)  # Package data into a dataset\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    model = HangmanLSTM().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training loop...\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):  # Loop over epochs\n",
        "        total_loss = 0\n",
        "        for word_batch, guessed_batch, target_batch in dataloader:  # Loop over batches\n",
        "            word_batch = word_batch.to(device)  # Send to device\n",
        "            guessed_batch = guessed_batch.to(device)  # Send to device\n",
        "            target_batch = target_batch.to(device)  # Send to device\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(word_batch, guessed_batch)  # Predict outputs\n",
        "            loss = criterion(outputs, target_batch)  # Calculate loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update weights\n",
        "            total_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)  # Calculate average loss\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")  # Print loss for this epoch\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the word list from file\n",
        "    words_file = \"words.txt\"\n",
        "    try:\n",
        "        with open(words_file, 'r') as f:\n",
        "            word_list = [line.strip().lower() for line in f if line.strip()]  # Read and clean up words\n",
        "        print(f\"Loaded {len(word_list)} words from {words_file}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {words_file} not found.\")  # Handle file not found error\n",
        "        exit(1)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting non-parallel training...\")\n",
        "    start_time = time.time()\n",
        "    train_model(word_list, model_path=\"large_hangman_model_cpu_non_parallel.pth\", num_samples=20000, epochs=25, batch_size=32, lr=0.001)\n",
        "    end_time = time.time()\n",
        "    print(f\"Non-parallel training completed in {end_time - start_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMusFOvKaexS",
        "outputId": "da8dab91-d539-4aa4-a593-da901498351c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 852 words from words.txt.\n",
            "Starting non-parallel training...\n",
            "Using device: cpu\n",
            "Generating training data...\n",
            "Generating training data...\n",
            "Generated 244178 training samples.\n",
            "Data generation took 0.96 seconds.\n",
            "Preparing data tensors...\n",
            "Creating DataLoader...\n",
            "Starting training loop...\n",
            "Epoch 1/25, Loss: 2.9312\n",
            "Epoch 2/25, Loss: 2.8293\n",
            "Epoch 3/25, Loss: 2.8067\n",
            "Epoch 4/25, Loss: 2.8014\n",
            "Epoch 5/25, Loss: 2.7994\n",
            "Epoch 6/25, Loss: 2.7974\n",
            "Epoch 7/25, Loss: 2.7952\n",
            "Epoch 8/25, Loss: 2.7935\n",
            "Epoch 9/25, Loss: 2.7917\n",
            "Epoch 10/25, Loss: 2.7904\n",
            "Epoch 11/25, Loss: 2.7890\n",
            "Epoch 12/25, Loss: 2.7878\n",
            "Epoch 13/25, Loss: 2.7867\n",
            "Epoch 14/25, Loss: 2.7853\n",
            "Epoch 15/25, Loss: 2.7840\n",
            "Epoch 16/25, Loss: 2.7823\n",
            "Epoch 17/25, Loss: 2.7806\n",
            "Epoch 18/25, Loss: 2.7788\n",
            "Epoch 19/25, Loss: 2.7765\n",
            "Epoch 20/25, Loss: 2.7751\n",
            "Epoch 21/25, Loss: 2.7714\n",
            "Epoch 22/25, Loss: 2.7688\n",
            "Epoch 23/25, Loss: 2.7661\n",
            "Epoch 24/25, Loss: 2.7627\n",
            "Epoch 25/25, Loss: 2.7589\n",
            "Model saved to large_hangman_model_cpu_non_parallel.pth\n",
            "Non-parallel training completed in 3519.24 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Junfeng Li\n",
        "\n",
        "Project Overview (Parallel Model):\n",
        "1. This implementation extends the non-parallel Hangman training model by introducing parallelized data generation on Google Colab.\n",
        "2. Training data generation is distributed across multiple processes using Python's multiprocessing module.\n",
        "3. The model architecture, training pipeline, and dataset handling remain consistent with the non-parallel version.\n",
        "\n",
        "Key Learnings and Thought Process:\n",
        "- Parallel Processing: Applied multiprocessing to accelerate training data generation, see if it is significantly reducing execution time on Google Colab.\n",
        "- Scalability: Designed the parallelization to handle larger datasets efficiently.\n",
        "- Performance Comparison: Demonstrated the advantages of parallel processing over sequential data generation, especially for time-intensive tasks.\n",
        "\n",
        "Challenges and Future Improvements:\n",
        "1. Managing inter-process communication and memory usage for even larger datasets can be further optimized.\n",
        "2. While the focus here was on data generation, future iterations could explore parallelism in training or model evaluation.\n",
        "\n",
        "Some refrence：\n",
        "1.https://docs.python.org/3/library/multiprocessing.html\n",
        "2.https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html\n",
        "3.https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html\n",
        "4.https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "5.https://www.geeksforgeeks.org/loading-data-in-pytorch/\n",
        "6.https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "\n",
        "Note: Luke managed the local testing and might modify something based on his own preference\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "from new_ai_player import HangmanLSTM\n",
        "\n",
        "def generate_training_data(word_list, num_samples, max_word_length=10):\n",
        "    \"\"\"\n",
        "    Generate training data based on a word list.\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of words to create training examples.\n",
        "        num_samples (int): The number of training examples to generate.\n",
        "        max_word_length (int): The maximum length of words to encode.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of training examples containing word state, guessed letters, and target letter.\n",
        "    \"\"\"\n",
        "    data = []  # This list will store all the training samples\n",
        "\n",
        "    for _ in range(num_samples):  # Repeat for the number of samples needed\n",
        "        word = random.choice(word_list)  # Choose a random word from the list\n",
        "        word_letters = set(word)  # Get all unique letters in the word\n",
        "        guessed_letters = set()  # Start with an empty set of guessed letters\n",
        "        obscured_word = ['_' for _ in word]  # Initially, the word is completely hidden\n",
        "\n",
        "        # Keep guessing letters until all unique letters are guessed\n",
        "        while len(guessed_letters) < len(word_letters):\n",
        "            next_letter = random.choice(list(word_letters - guessed_letters))  # Pick an unguessed letter\n",
        "            guessed_letters.add(next_letter)  # Add it to the guessed letters\n",
        "\n",
        "            # Reveal guessed letters in the obscured word\n",
        "            for i, char in enumerate(word):\n",
        "                if char in guessed_letters:\n",
        "                    obscured_word[i] = char\n",
        "\n",
        "            # Encode the current word state and guessed letters\n",
        "            word_input = encode_word_state(''.join(obscured_word), max_word_length)\n",
        "            guessed_input = encode_guessed_letters(guessed_letters)\n",
        "            target_letter = ord(next_letter) - ord('a')  # Convert the letter to an index (0-25)\n",
        "\n",
        "            # Balance the dataset by repeating incomplete samples\n",
        "            if '_' in ''.join(obscured_word):  # Word not fully revealed\n",
        "                for _ in range(3):  # Add this sample 3 times\n",
        "                    data.append((word_input, guessed_input, target_letter))\n",
        "            else:\n",
        "                data.append((word_input, guessed_input, target_letter))  # Add fully revealed word sample\n",
        "\n",
        "    return data  # Return the training examples\n",
        "\n",
        "def encode_word_state(word_display, max_word_length):\n",
        "    \"\"\"\n",
        "    Convert the word's current state into a one-hot encoding matrix.\n",
        "\n",
        "    Args:\n",
        "        word_display (str): The current view of the word (e.g., \"_ppl_\" for \"apple\").\n",
        "        max_word_length (int): Maximum length of the word to encode.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A matrix with one-hot encoding of the word's current state.\n",
        "    \"\"\"\n",
        "    word_vector = np.zeros((max_word_length, 27))  # Create a matrix of zeros\n",
        "    for i, char in enumerate(word_display[:max_word_length]):  # Process each character\n",
        "        if char == '_':\n",
        "            word_vector[i, 26] = 1  # Mark blanks in the last column\n",
        "        elif 'a' <= char <= 'z':\n",
        "            word_vector[i, ord(char) - ord('a')] = 1  # Mark the column for the corresponding letter\n",
        "    return word_vector  # Return the one-hot encoded matrix\n",
        "\n",
        "def encode_guessed_letters(guessed_letters):\n",
        "    \"\"\"\n",
        "    Convert guessed letters into a one-hot vector.\n",
        "\n",
        "    Args:\n",
        "        guessed_letters (set): Letters guessed so far.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A one-hot encoded vector of size 26.\n",
        "    \"\"\"\n",
        "    guessed_vector = np.zeros(26)  # Create a zero vector of size 26\n",
        "    for letter in guessed_letters:  # Process each guessed letter\n",
        "        guessed_vector[ord(letter) - ord('a')] = 1  # Mark the corresponding position\n",
        "    return guessed_vector  # Return the one-hot encoded vector\n",
        "\n",
        "def parallel_generate_training_data(args):\n",
        "    \"\"\"\n",
        "    Wrapper function for multiprocessing to generate training data in parallel.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): A tuple containing word list, sample count, and max word length.\n",
        "\n",
        "    Returns:\n",
        "        list: Training data generated by the worker.\n",
        "    \"\"\"\n",
        "    word_list, num_samples, max_word_length = args\n",
        "    return generate_training_data(word_list, num_samples, max_word_length)\n",
        "\n",
        "def train_model_parallel(word_list, model_path='large_hangman_model_cpu_parallel.pth', num_samples=10000, epochs=25, batch_size=32, lr=0.001, num_workers=4):\n",
        "    \"\"\"\n",
        "    Train the HangmanLSTM model with parallel data generation.\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of words for training.\n",
        "        model_path (str): File path to save the trained model.\n",
        "        num_samples (int): Number of training examples to generate.\n",
        "        epochs (int): Number of training iterations.\n",
        "        batch_size (int): Number of samples in each training batch.\n",
        "        lr (float): Learning rate for optimization.\n",
        "        num_workers (int): Number of processes for parallel data generation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Choose GPU if available\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Generate training data in parallel\n",
        "    print(\"Generating training data in parallel...\")\n",
        "    chunk_size = num_samples // num_workers  # Divide samples evenly among workers\n",
        "    pool_args = [(word_list, chunk_size, 10) for _ in range(num_workers)]  # Arguments for each process\n",
        "\n",
        "    with Pool(num_workers) as pool:\n",
        "        data_chunks = pool.map(parallel_generate_training_data, pool_args)  # Generate data in parallel\n",
        "\n",
        "    # Combine all data chunks into a single dataset\n",
        "    data = [item for chunk in data_chunks for item in chunk]\n",
        "    print(f\"Generated {len(data)} training samples.\")\n",
        "\n",
        "    # Convert data into tensors\n",
        "    print(\"Preparing data tensors...\")\n",
        "    inputs_word = torch.tensor(np.array([item[0] for item in data]), dtype=torch.float32)\n",
        "    inputs_guessed = torch.tensor(np.array([item[1] for item in data]), dtype=torch.float32)\n",
        "    targets = torch.tensor(np.array([item[2] for item in data]), dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader for batching during training\n",
        "    print(\"Creating DataLoader...\")\n",
        "    dataset = torch.utils.data.TensorDataset(inputs_word, inputs_guessed, targets)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    # Set up the model, optimizer, and loss function\n",
        "    model = HangmanLSTM().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training loop...\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):  # Iterate through epochs\n",
        "        total_loss = 0\n",
        "        for word_batch, guessed_batch, target_batch in dataloader:  # Process each batch\n",
        "            word_batch = word_batch.to(device)\n",
        "            guessed_batch = guessed_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(word_batch, guessed_batch)  # Get predictions\n",
        "            loss = criterion(outputs, target_batch)  # Calculate loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update model parameters\n",
        "            total_loss += loss.item()  # Accumulate total loss\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)  # Calculate average loss\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")  # Print loss for this epoch\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the word list from file\n",
        "    words_file = \"words.txt\"\n",
        "    try:\n",
        "        with open(words_file, 'r') as f:\n",
        "            word_list = [line.strip().lower() for line in f if line.strip()]  # Read words from file\n",
        "        print(f\"Loaded {len(word_list)} words from {words_file}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {words_file} not found.\")  # Handle missing file\n",
        "        exit(1)\n",
        "\n",
        "    # Train the model using parallel processing\n",
        "    print(\"Starting parallel training...\")\n",
        "    start_time = time.time()\n",
        "    train_model_parallel(word_list, model_path=\"large_hangman_model_cpu_parallel.pth\", num_samples=20000, epochs=25, batch_size=32, lr=0.001, num_workers=4)\n",
        "    end_time = time.time()\n",
        "    print(f\"Parallel training completed in {end_time - start_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1NzMyNKoVZt",
        "outputId": "c4a55a3d-0761-48df-a8a8-0448acb119fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 852 words from words.txt.\n",
            "Starting parallel training...\n",
            "Using device: cpu\n",
            "Generating training data in parallel...\n",
            "Generated 244235 training samples.\n",
            "Preparing data tensors...\n",
            "Creating DataLoader...\n",
            "Starting training loop...\n",
            "Epoch 1/25, Loss: 2.9299\n",
            "Epoch 2/25, Loss: 2.8281\n",
            "Epoch 3/25, Loss: 2.8065\n",
            "Epoch 4/25, Loss: 2.8009\n",
            "Epoch 5/25, Loss: 2.7973\n",
            "Epoch 6/25, Loss: 2.7951\n",
            "Epoch 7/25, Loss: 2.7934\n",
            "Epoch 8/25, Loss: 2.7914\n",
            "Epoch 9/25, Loss: 2.7901\n",
            "Epoch 10/25, Loss: 2.7884\n",
            "Epoch 11/25, Loss: 2.7868\n",
            "Epoch 12/25, Loss: 2.7855\n",
            "Epoch 13/25, Loss: 2.7839\n",
            "Epoch 14/25, Loss: 2.7827\n",
            "Epoch 15/25, Loss: 2.7814\n",
            "Epoch 16/25, Loss: 2.7803\n",
            "Epoch 17/25, Loss: 2.7792\n",
            "Epoch 18/25, Loss: 2.7780\n",
            "Epoch 19/25, Loss: 2.7772\n",
            "Epoch 20/25, Loss: 2.7763\n",
            "Epoch 21/25, Loss: 2.7751\n",
            "Epoch 22/25, Loss: 2.7744\n",
            "Epoch 23/25, Loss: 2.7731\n",
            "Epoch 24/25, Loss: 2.7722\n",
            "Epoch 25/25, Loss: 2.7719\n",
            "Model saved to large_hangman_model_cpu_parallel.pth\n",
            "Parallel training completed in 3082.83 seconds.\n"
          ]
        }
      ]
    }
  ]
}