{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaJgCz8A96Fa",
        "outputId": "1742f423-69df-40a0-9e15-9df02dbc4a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_ai_player.py  sample_data  words.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from new_ai_player import HangmanLSTM\n",
        "\n",
        "def generate_training_data(word_list, num_samples, max_word_length=10):\n",
        "    \"\"\"Generate training data based on word list.\"\"\"\n",
        "    print(\"Generating training data...\")\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        word = random.choice(word_list)\n",
        "        word_letters = set(word)\n",
        "        guessed_letters = set()\n",
        "        obscured_word = ['_' for _ in word]\n",
        "\n",
        "        while len(guessed_letters) < len(word_letters):\n",
        "            next_letter = random.choice(list(word_letters - guessed_letters))\n",
        "            guessed_letters.add(next_letter)\n",
        "\n",
        "            for i, char in enumerate(word):\n",
        "                if char in guessed_letters:\n",
        "                    obscured_word[i] = char\n",
        "\n",
        "            word_input = encode_word_state(''.join(obscured_word), max_word_length)\n",
        "            guessed_input = encode_guessed_letters(guessed_letters)\n",
        "            target_letter = ord(next_letter) - ord('a')\n",
        "\n",
        "            if '_' in ''.join(obscured_word):\n",
        "                for _ in range(3):  # Repeat to balance the dataset\n",
        "                    data.append((word_input, guessed_input, target_letter))\n",
        "            else:\n",
        "                data.append((word_input, guessed_input, target_letter))\n",
        "    print(f\"Generated {len(data)} training samples.\")\n",
        "    return data\n",
        "\n",
        "def encode_word_state(word_display, max_word_length):\n",
        "    \"\"\"Encode word state as a one-hot matrix.\"\"\"\n",
        "    word_vector = np.zeros((max_word_length, 27))  # 27: 26 letters + 1 for '_'\n",
        "    for i, char in enumerate(word_display[:max_word_length]):\n",
        "        if char == '_':\n",
        "            word_vector[i, 26] = 1  # Represent blanks as the 27th feature\n",
        "        elif 'a' <= char <= 'z':\n",
        "            word_vector[i, ord(char) - ord('a')] = 1\n",
        "    return word_vector\n",
        "\n",
        "def encode_guessed_letters(guessed_letters):\n",
        "    \"\"\"Encode guessed letters as a one-hot vector.\"\"\"\n",
        "    guessed_vector = np.zeros(26)\n",
        "    for letter in guessed_letters:\n",
        "        guessed_vector[ord(letter) - ord('a')] = 1\n",
        "    return guessed_vector\n",
        "\n",
        "def train_model(word_list, model_path='large_hangman_model_normal_parallel.pth', num_samples=10000, epochs=25, batch_size=32, lr=0.001):\n",
        "    \"\"\"Train the HangmanLSTM model.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Generate training data\n",
        "    print(\"Generating training data...\")\n",
        "    start_time_data = time.time()\n",
        "    data = generate_training_data(word_list, num_samples)\n",
        "    end_time_data = time.time()\n",
        "    print(f\"Data generation took {end_time_data - start_time_data:.2f} seconds.\")\n",
        "\n",
        "    # Prepare tensors\n",
        "    print(\"Preparing data tensors...\")\n",
        "    inputs_word = torch.tensor(np.array([item[0] for item in data]), dtype=torch.float32)\n",
        "    inputs_guessed = torch.tensor(np.array([item[1] for item in data]), dtype=torch.float32)\n",
        "    targets = torch.tensor(np.array([item[2] for item in data]), dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader\n",
        "    print(\"Creating DataLoader...\")\n",
        "    dataset = torch.utils.data.TensorDataset(inputs_word, inputs_guessed, targets)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    # Initialize model\n",
        "    model = HangmanLSTM().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training loop...\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for word_batch, guessed_batch, target_batch in dataloader:\n",
        "            word_batch = word_batch.to(device)\n",
        "            guessed_batch = guessed_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(word_batch, guessed_batch)\n",
        "            loss = criterion(outputs, target_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the word list from file\n",
        "    words_file = \"words.txt\"\n",
        "    try:\n",
        "        with open(words_file, 'r') as f:\n",
        "            word_list = [line.strip().lower() for line in f if line.strip()]\n",
        "        print(f\"Loaded {len(word_list)} words from {words_file}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {words_file} not found.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting non-parallel training...\")\n",
        "    start_time = time.time()\n",
        "    train_model(word_list, model_path=\"large_hangman_model_normal_parallel.pth\", num_samples=20000, epochs=25, batch_size=32, lr=0.001)\n",
        "    end_time = time.time()\n",
        "    print(f\"Non-parallel training completed in {end_time - start_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEn50PqtGTGg",
        "outputId": "6dbbff25-3c70-4df0-ecc3-45348d68ddfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 852 words from words.txt.\n",
            "Starting non-parallel training...\n",
            "Using device: cuda\n",
            "Generating training data...\n",
            "Generating training data...\n",
            "Generated 245249 training samples.\n",
            "Data generation took 0.74 seconds.\n",
            "Preparing data tensors...\n",
            "Creating DataLoader...\n",
            "Starting training loop...\n",
            "Epoch 1/25, Loss: 2.9377\n",
            "Epoch 2/25, Loss: 2.8326\n",
            "Epoch 3/25, Loss: 2.8113\n",
            "Epoch 4/25, Loss: 2.8065\n",
            "Epoch 5/25, Loss: 2.8026\n",
            "Epoch 6/25, Loss: 2.7997\n",
            "Epoch 7/25, Loss: 2.7980\n",
            "Epoch 8/25, Loss: 2.7950\n",
            "Epoch 9/25, Loss: 2.7932\n",
            "Epoch 10/25, Loss: 2.7908\n",
            "Epoch 11/25, Loss: 2.7880\n",
            "Epoch 12/25, Loss: 2.7850\n",
            "Epoch 13/25, Loss: 2.7821\n",
            "Epoch 14/25, Loss: 2.7799\n",
            "Epoch 15/25, Loss: 2.7759\n",
            "Epoch 16/25, Loss: 2.7728\n",
            "Epoch 17/25, Loss: 2.7695\n",
            "Epoch 18/25, Loss: 2.7662\n",
            "Epoch 19/25, Loss: 2.7627\n",
            "Epoch 20/25, Loss: 2.7589\n",
            "Epoch 21/25, Loss: 2.7559\n",
            "Epoch 22/25, Loss: 2.7519\n",
            "Epoch 23/25, Loss: 2.7474\n",
            "Epoch 24/25, Loss: 2.7439\n",
            "Epoch 25/25, Loss: 2.7405\n",
            "Model saved to large_hangman_model_normal_parallel.pth\n",
            "Non-parallel training completed in 602.37 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "from new_ai_player import HangmanLSTM\n",
        "\n",
        "def generate_training_data(word_list, num_samples, max_word_length=10):\n",
        "    \"\"\"Generate training data based on word list.\"\"\"\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        word = random.choice(word_list)\n",
        "        word_letters = set(word)\n",
        "        guessed_letters = set()\n",
        "        obscured_word = ['_' for _ in word]\n",
        "\n",
        "        while len(guessed_letters) < len(word_letters):\n",
        "            next_letter = random.choice(list(word_letters - guessed_letters))\n",
        "            guessed_letters.add(next_letter)\n",
        "\n",
        "            for i, char in enumerate(word):\n",
        "                if char in guessed_letters:\n",
        "                    obscured_word[i] = char\n",
        "\n",
        "            word_input = encode_word_state(''.join(obscured_word), max_word_length)\n",
        "            guessed_input = encode_guessed_letters(guessed_letters)\n",
        "            target_letter = ord(next_letter) - ord('a')\n",
        "\n",
        "            if '_' in ''.join(obscured_word):\n",
        "                for _ in range(3):  # Repeat to balance the dataset\n",
        "                    data.append((word_input, guessed_input, target_letter))\n",
        "            else:\n",
        "                data.append((word_input, guessed_input, target_letter))\n",
        "    return data\n",
        "\n",
        "def encode_word_state(word_display, max_word_length):\n",
        "    \"\"\"Encode word state as a one-hot matrix.\"\"\"\n",
        "    word_vector = np.zeros((max_word_length, 27))  # 27: 26 letters + 1 for '_'\n",
        "    for i, char in enumerate(word_display[:max_word_length]):\n",
        "        if char == '_':\n",
        "            word_vector[i, 26] = 1  # Represent blanks as the 27th feature\n",
        "        elif 'a' <= char <= 'z':\n",
        "            word_vector[i, ord(char) - ord('a')] = 1\n",
        "    return word_vector\n",
        "\n",
        "def encode_guessed_letters(guessed_letters):\n",
        "    \"\"\"Encode guessed letters as a one-hot vector.\"\"\"\n",
        "    guessed_vector = np.zeros(26)\n",
        "    for letter in guessed_letters:\n",
        "        guessed_vector[ord(letter) - ord('a')] = 1\n",
        "    return guessed_vector\n",
        "\n",
        "def parallel_generate_training_data(args):\n",
        "    \"\"\"Wrapper for multiprocessing.\"\"\"\n",
        "    word_list, num_samples, max_word_length = args\n",
        "    return generate_training_data(word_list, num_samples, max_word_length)\n",
        "\n",
        "def train_model_parallel(word_list, model_path='large_hangman_model_super_parallel.pth', num_samples=10000, epochs=25, batch_size=32, lr=0.001, num_workers=4):\n",
        "    \"\"\"Train the HangmanLSTM model with parallel data generation.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Parallel data generation\n",
        "    print(\"Generating training data in parallel...\")\n",
        "    chunk_size = num_samples // num_workers\n",
        "    pool_args = [(word_list, chunk_size, 10) for _ in range(num_workers)]\n",
        "\n",
        "    with Pool(num_workers) as pool:\n",
        "        data_chunks = pool.map(parallel_generate_training_data, pool_args)\n",
        "\n",
        "    data = [item for chunk in data_chunks for item in chunk]  # Flatten the list\n",
        "    print(f\"Generated {len(data)} training samples.\")\n",
        "\n",
        "    # Prepare tensors\n",
        "    print(\"Preparing data tensors...\")\n",
        "    inputs_word = torch.tensor(np.array([item[0] for item in data]), dtype=torch.float32)\n",
        "    inputs_guessed = torch.tensor(np.array([item[1] for item in data]), dtype=torch.float32)\n",
        "    targets = torch.tensor(np.array([item[2] for item in data]), dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader\n",
        "    print(\"Creating DataLoader...\")\n",
        "    dataset = torch.utils.data.TensorDataset(inputs_word, inputs_guessed, targets)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    # Initialize model\n",
        "    model = HangmanLSTM().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training loop...\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for word_batch, guessed_batch, target_batch in dataloader:\n",
        "            word_batch = word_batch.to(device)\n",
        "            guessed_batch = guessed_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(word_batch, guessed_batch)\n",
        "            loss = criterion(outputs, target_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the word list from file\n",
        "    words_file = \"words.txt\"\n",
        "    try:\n",
        "        with open(words_file, 'r') as f:\n",
        "            word_list = [line.strip().lower() for line in f if line.strip()]\n",
        "        print(f\"Loaded {len(word_list)} words from {words_file}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {words_file} not found.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Train the model in parallel\n",
        "    print(\"Starting parallel training...\")\n",
        "    start_time = time.time()\n",
        "    train_model_parallel(word_list, model_path=\"large_hangman_model_super_parallel.pth\", num_samples=20000, epochs=25, batch_size=32, lr=0.001, num_workers=4)\n",
        "    end_time = time.time()\n",
        "    print(f\"Parallel training completed in {end_time - start_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMBiIRBjKArE",
        "outputId": "c7d7ed5f-80e7-41f3-84de-3a6fa8e2e2ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 852 words from words.txt.\n",
            "Starting parallel training...\n",
            "Using device: cuda\n",
            "Generating training data in parallel...\n",
            "Generated 244091 training samples.\n",
            "Preparing data tensors...\n",
            "Creating DataLoader...\n",
            "Starting training loop...\n",
            "Epoch 1/25, Loss: 2.9479\n",
            "Epoch 2/25, Loss: 2.8439\n",
            "Epoch 3/25, Loss: 2.8109\n",
            "Epoch 4/25, Loss: 2.8061\n",
            "Epoch 5/25, Loss: 2.8026\n",
            "Epoch 6/25, Loss: 2.7988\n",
            "Epoch 7/25, Loss: 2.7963\n",
            "Epoch 8/25, Loss: 2.7958\n",
            "Epoch 9/25, Loss: 2.7946\n",
            "Epoch 10/25, Loss: 2.7910\n",
            "Epoch 11/25, Loss: 2.7888\n",
            "Epoch 12/25, Loss: 2.7840\n",
            "Epoch 13/25, Loss: 2.7802\n",
            "Epoch 14/25, Loss: 2.7763\n",
            "Epoch 15/25, Loss: 2.7718\n",
            "Epoch 16/25, Loss: 2.7674\n",
            "Epoch 17/25, Loss: 2.7634\n",
            "Epoch 18/25, Loss: 2.7574\n",
            "Epoch 19/25, Loss: 2.7520\n",
            "Epoch 20/25, Loss: 2.7473\n",
            "Epoch 21/25, Loss: 2.7417\n",
            "Epoch 22/25, Loss: 2.7378\n",
            "Epoch 23/25, Loss: 2.7330\n",
            "Epoch 24/25, Loss: 2.7285\n",
            "Epoch 25/25, Loss: 2.7239\n",
            "Model saved to large_hangman_model_super_parallel.pth\n",
            "Parallel training completed in 604.43 seconds.\n"
          ]
        }
      ]
    }
  ]
}