{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaJgCz8A96Fa",
    "outputId": "1742f423-69df-40a0-9e15-9df02dbc4a4e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new_ai_player.py  sample_data  words.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Junfeng Li\n",
    "\n",
    "Project Overview (Non-Parallel Model):\n",
    "1. This implementation trains a deep learning model for the Hangman game using PyTorch on Google Colab.\n",
    "2. Training data is generated dynamically by simulating the game process, encoding word states and guessed letters.\n",
    "3. The model utilizes an LSTM architecture to predict the next letter during the game.\n",
    "\n",
    "Key Learnings and Thought Process:\n",
    "- Data Generation: Experimented with methods to generate balanced training samples, ensuring a diverse dataset for effective learning.\n",
    "- Training Pipeline: Gained experience in setting up PyTorch pipelines, including tensor creation, DataLoader usage, and batch processing.\n",
    "- Model Design: Used LSTM to capture the sequence nature of the game, learning the relationship between word states and guessed letters.\n",
    "\n",
    "Challenges and Future Improvements:\n",
    "1. The data generation process is time-intensive for larger datasets, motivating the need for optimization.\n",
    "2. Further work could involve exploring alternative architectures or hyperparameter tuning to improve accuracy and efficiency.\n",
    "\n",
    "Some refrence：\n",
    "1.https://docs.python.org/3/library/multiprocessing.html\n",
    "2.https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html\n",
    "3.https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html\n",
    "4.https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "5.https://www.geeksforgeeks.org/loading-data-in-pytorch/\n",
    "6.https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "Note: Luke managed the local testing and might modify something based on his own preference\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from new_ai_player import HangmanLSTM\n",
    "\n",
    "def generate_training_data(word_list, num_samples, max_word_length=10):\n",
    "    \"\"\"\n",
    "    Generate training data based on a word list.\n",
    "\n",
    "    Args:\n",
    "        word_list (list): A list of words to create training examples.\n",
    "        num_samples (int): The number of training examples to generate.\n",
    "        max_word_length (int): The maximum length of words to handle.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of training examples containing word state, guessed letters, and target letter.\n",
    "    \"\"\"\n",
    "    print(\"Generating training data...\")\n",
    "    data = []  # This list will store all the training samples\n",
    "\n",
    "    for _ in range(num_samples):  # Repeat for the number of samples needed\n",
    "        word = random.choice(word_list)  # Randomly select a word from the list\n",
    "        word_letters = set(word)  # Get all unique letters in the word\n",
    "        guessed_letters = set()  # Start with an empty set of guessed letters\n",
    "        obscured_word = ['_' for _ in word]  # Initially, the word is completely hidden\n",
    "\n",
    "        # Keep guessing letters until all unique letters are guessed\n",
    "        while len(guessed_letters) < len(word_letters):\n",
    "            next_letter = random.choice(list(word_letters - guessed_letters))  # Pick an unguessed letter\n",
    "            guessed_letters.add(next_letter)  # Add it to the guessed letters\n",
    "\n",
    "            # Reveal guessed letters in the obscured word\n",
    "            for i, char in enumerate(word):\n",
    "                if char in guessed_letters:\n",
    "                    obscured_word[i] = char\n",
    "\n",
    "            # Encode the current word state and guessed letters\n",
    "            word_input = encode_word_state(''.join(obscured_word), max_word_length)\n",
    "            guessed_input = encode_guessed_letters(guessed_letters)\n",
    "            target_letter = ord(next_letter) - ord('a')  # Convert the letter to an index (0-25)\n",
    "\n",
    "            # Balance the dataset by repeating incomplete samples\n",
    "            if '_' in ''.join(obscured_word):  # Word not fully revealed\n",
    "                for _ in range(3):  # Add this sample 3 times\n",
    "                    data.append((word_input, guessed_input, target_letter))\n",
    "            else:\n",
    "                data.append((word_input, guessed_input, target_letter))  # Add fully revealed word sample\n",
    "\n",
    "    print(f\"Generated {len(data)} training samples.\")  # Print the total number of samples\n",
    "    return data  # Return the training examples\n",
    "\n",
    "def encode_word_state(word_display, max_word_length):\n",
    "    \"\"\"\n",
    "    Convert the word's current state into a one-hot encoding matrix.\n",
    "\n",
    "    Args:\n",
    "        word_display (str): The current view of the word (e.g., \"_ppl_\" for \"apple\").\n",
    "        max_word_length (int): Maximum length of the word to encode.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A matrix with one-hot encoding of the word's current state.\n",
    "    \"\"\"\n",
    "    word_vector = np.zeros((max_word_length, 27))  # Create a matrix of zeros\n",
    "    for i, char in enumerate(word_display[:max_word_length]):  # Process each character\n",
    "        if char == '_':\n",
    "            word_vector[i, 26] = 1  # Mark blanks in the last column\n",
    "        elif 'a' <= char <= 'z':\n",
    "            word_vector[i, ord(char) - ord('a')] = 1  # Mark the column for the corresponding letter\n",
    "    return word_vector  # Return the one-hot encoded matrix\n",
    "\n",
    "def encode_guessed_letters(guessed_letters):\n",
    "    \"\"\"\n",
    "    Convert guessed letters into a one-hot vector.\n",
    "\n",
    "    Args:\n",
    "        guessed_letters (set): Letters guessed so far.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A one-hot encoded vector of size 26.\n",
    "    \"\"\"\n",
    "    guessed_vector = np.zeros(26)  # Create a zero vector of size 26\n",
    "    for letter in guessed_letters:  # Process each guessed letter\n",
    "        guessed_vector[ord(letter) - ord('a')] = 1  # Mark the corresponding position\n",
    "    return guessed_vector  # Return the one-hot encoded vector\n",
    "\n",
    "def train_model(word_list, model_path='large_hangman_model_normal_parallel.pth', num_samples=10000, epochs=25, batch_size=32, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train the HangmanLSTM model.\n",
    "\n",
    "    Args:\n",
    "        word_list (list): A list of words for training.\n",
    "        model_path (str): File path to save the trained model.\n",
    "        num_samples (int): Number of training examples to generate.\n",
    "        epochs (int): Number of training iterations.\n",
    "        batch_size (int): Number of samples in each training batch.\n",
    "        lr (float): Learning rate for optimization.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Choose GPU if available\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Generate training data\n",
    "    print(\"Generating training data...\")\n",
    "    start_time_data = time.time()\n",
    "    data = generate_training_data(word_list, num_samples)  # Create training examples\n",
    "    end_time_data = time.time()\n",
    "    print(f\"Data generation took {end_time_data - start_time_data:.2f} seconds.\")\n",
    "\n",
    "    # Prepare tensors\n",
    "    print(\"Preparing data tensors...\")\n",
    "    inputs_word = torch.tensor(np.array([item[0] for item in data]), dtype=torch.float32)  # Word inputs\n",
    "    inputs_guessed = torch.tensor(np.array([item[1] for item in data]), dtype=torch.float32)  # Guessed letters\n",
    "    targets = torch.tensor(np.array([item[2] for item in data]), dtype=torch.long)  # Target letters\n",
    "\n",
    "    # Create DataLoader\n",
    "    print(\"Creating DataLoader...\")\n",
    "    dataset = torch.utils.data.TensorDataset(inputs_word, inputs_guessed, targets)  # Package data into a dataset\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = HangmanLSTM().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training loop...\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):  # Iterate through epochs\n",
    "        total_loss = 0\n",
    "        for word_batch, guessed_batch, target_batch in dataloader:  # Process each batch\n",
    "            word_batch = word_batch.to(device)\n",
    "            guessed_batch = guessed_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs = model(word_batch, guessed_batch)  # Get predictions\n",
    "            loss = criterion(outputs, target_batch)  # Calculate loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)  # Calculate average loss\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")  # Print loss for this epoch\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the word list from file\n",
    "    words_file = \"words.txt\"\n",
    "    try:\n",
    "        with open(words_file, 'r') as f:\n",
    "            word_list = [line.strip().lower() for line in f if line.strip()]  # Read words from file\n",
    "        print(f\"Loaded {len(word_list)} words from {words_file}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {words_file} not found.\")  # Handle missing file\n",
    "        exit(1)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting non-parallel training...\")\n",
    "    start_time = time.time()\n",
    "    train_model(word_list, model_path=\"trained_models/large_hangman_model_normal_parallel.pth\", num_samples=20000, epochs=25, batch_size=32, lr=0.001)\n",
    "    end_time = time.time()\n",
    "    print(f\"Non-parallel training completed in {end_time - start_time:.2f} seconds.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEn50PqtGTGg",
    "outputId": "6dbbff25-3c70-4df0-ecc3-45348d68ddfe"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 852 words from words.txt.\n",
      "Starting non-parallel training...\n",
      "Using device: cuda\n",
      "Generating training data...\n",
      "Generating training data...\n",
      "Generated 245249 training samples.\n",
      "Data generation took 0.74 seconds.\n",
      "Preparing data tensors...\n",
      "Creating DataLoader...\n",
      "Starting training loop...\n",
      "Epoch 1/25, Loss: 2.9377\n",
      "Epoch 2/25, Loss: 2.8326\n",
      "Epoch 3/25, Loss: 2.8113\n",
      "Epoch 4/25, Loss: 2.8065\n",
      "Epoch 5/25, Loss: 2.8026\n",
      "Epoch 6/25, Loss: 2.7997\n",
      "Epoch 7/25, Loss: 2.7980\n",
      "Epoch 8/25, Loss: 2.7950\n",
      "Epoch 9/25, Loss: 2.7932\n",
      "Epoch 10/25, Loss: 2.7908\n",
      "Epoch 11/25, Loss: 2.7880\n",
      "Epoch 12/25, Loss: 2.7850\n",
      "Epoch 13/25, Loss: 2.7821\n",
      "Epoch 14/25, Loss: 2.7799\n",
      "Epoch 15/25, Loss: 2.7759\n",
      "Epoch 16/25, Loss: 2.7728\n",
      "Epoch 17/25, Loss: 2.7695\n",
      "Epoch 18/25, Loss: 2.7662\n",
      "Epoch 19/25, Loss: 2.7627\n",
      "Epoch 20/25, Loss: 2.7589\n",
      "Epoch 21/25, Loss: 2.7559\n",
      "Epoch 22/25, Loss: 2.7519\n",
      "Epoch 23/25, Loss: 2.7474\n",
      "Epoch 24/25, Loss: 2.7439\n",
      "Epoch 25/25, Loss: 2.7405\n",
      "Model saved to large_hangman_model_normal_parallel.pth\n",
      "Non-parallel training completed in 602.37 seconds.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Junfeng Li\n",
    "\n",
    "Project Overview (Parallel Model):\n",
    "1. This implementation extends the non-parallel Hangman training model by introducing parallelized data generation on Google Colab.\n",
    "2. Training data generation is distributed across multiple processes using Python's multiprocessing module.\n",
    "3. The model architecture, training pipeline, and dataset handling remain consistent with the non-parallel version.\n",
    "\n",
    "Key Learnings and Thought Process:\n",
    "- Parallel Processing: Applied multiprocessing to accelerate training data generation, see if it is significantly reducing execution time on Google Colab.\n",
    "- Scalability: Designed the parallelization to handle larger datasets efficiently.\n",
    "- Performance Comparison: Demonstrated the advantages of parallel processing over sequential data generation, especially for time-intensive tasks.\n",
    "\n",
    "Challenges and Future Improvements:\n",
    "1. Managing inter-process communication and memory usage for even larger datasets can be further optimized.\n",
    "2. While the focus here was on data generation, future iterations could explore parallelism in training or model evaluation.\n",
    "\n",
    "Some refrence：\n",
    "1.https://docs.python.org/3/library/multiprocessing.html\n",
    "2.https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html\n",
    "3.https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html\n",
    "4.https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "5.https://www.geeksforgeeks.org/loading-data-in-pytorch/\n",
    "6.https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "Note: Luke managed the local testing and might modify something based on his own preference\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from new_ai_player import HangmanLSTM\n",
    "\n",
    "def generate_training_data(word_list, num_samples, max_word_length=10):\n",
    "    \"\"\"\n",
    "    Generate training data based on a word list.\n",
    "\n",
    "    Args:\n",
    "        word_list (list): List of words for creating training examples.\n",
    "        num_samples (int): Number of training examples to generate.\n",
    "        max_word_length (int): Maximum word length to consider.\n",
    "\n",
    "    Returns:\n",
    "        list: Training examples containing word state, guessed letters, and target letter.\n",
    "    \"\"\"\n",
    "    data = []  # This list will store all training samples\n",
    "\n",
    "    for _ in range(num_samples):  # Loop for generating the specified number of examples\n",
    "        word = random.choice(word_list)  # Randomly select a word from the list\n",
    "        word_letters = set(word)  # Extract all unique letters in the word\n",
    "        guessed_letters = set()  # Start with no letters guessed\n",
    "        obscured_word = ['_' for _ in word]  # Represent the word as hidden initially\n",
    "\n",
    "        # Keep guessing letters until all unique letters are guessed\n",
    "        while len(guessed_letters) < len(word_letters):\n",
    "            next_letter = random.choice(list(word_letters - guessed_letters))  # Choose an unguessed letter\n",
    "            guessed_letters.add(next_letter)  # Mark the letter as guessed\n",
    "\n",
    "            # Update the hidden word to reveal guessed letters\n",
    "            for i, char in enumerate(word):\n",
    "                if char in guessed_letters:\n",
    "                    obscured_word[i] = char\n",
    "\n",
    "            # Encode the current word state and guessed letters\n",
    "            word_input = encode_word_state(''.join(obscured_word), max_word_length)\n",
    "            guessed_input = encode_guessed_letters(guessed_letters)\n",
    "            target_letter = ord(next_letter) - ord('a')  # Convert letter to index (0-25)\n",
    "\n",
    "            # Balance the dataset by repeating incomplete examples\n",
    "            if '_' in ''.join(obscured_word):  # If the word is not fully guessed\n",
    "                for _ in range(3):  # Add the example 3 times to balance the dataset\n",
    "                    data.append((word_input, guessed_input, target_letter))\n",
    "            else:  # Fully guessed word\n",
    "                data.append((word_input, guessed_input, target_letter))\n",
    "\n",
    "    return data  # Return the generated training examples\n",
    "\n",
    "def encode_word_state(word_display, max_word_length):\n",
    "    \"\"\"\n",
    "    Convert the current state of the word into a one-hot encoded matrix.\n",
    "\n",
    "    Args:\n",
    "        word_display (str): Current view of the word (e.g., \"_ppl_\" for \"apple\").\n",
    "        max_word_length (int): Maximum number of characters to encode.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A matrix representing the one-hot encoding of the word.\n",
    "    \"\"\"\n",
    "    word_vector = np.zeros((max_word_length, 27))  # Initialize a matrix of zeros\n",
    "    for i, char in enumerate(word_display[:max_word_length]):  # Process each character\n",
    "        if char == '_':\n",
    "            word_vector[i, 26] = 1  # Mark blanks in the last column\n",
    "        elif 'a' <= char <= 'z':\n",
    "            word_vector[i, ord(char) - ord('a')] = 1  # Mark the corresponding letter's column\n",
    "    return word_vector  # Return the encoded matrix\n",
    "\n",
    "def encode_guessed_letters(guessed_letters):\n",
    "    \"\"\"\n",
    "    Convert guessed letters into a one-hot encoded vector.\n",
    "\n",
    "    Args:\n",
    "        guessed_letters (set): Set of guessed letters.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A one-hot encoded vector of size 26.\n",
    "    \"\"\"\n",
    "    guessed_vector = np.zeros(26)  # Initialize a zero vector of size 26\n",
    "    for letter in guessed_letters:  # Iterate through the guessed letters\n",
    "        guessed_vector[ord(letter) - ord('a')] = 1  # Mark the corresponding position\n",
    "    return guessed_vector  # Return the encoded vector\n",
    "\n",
    "def parallel_generate_training_data(args):\n",
    "    \"\"\"\n",
    "    Wrapper function for multiprocessing to generate training data in parallel.\n",
    "\n",
    "    Args:\n",
    "        args (tuple): Contains word list, sample count, and max word length.\n",
    "\n",
    "    Returns:\n",
    "        list: Training data generated by the worker process.\n",
    "    \"\"\"\n",
    "    word_list, num_samples, max_word_length = args\n",
    "    return generate_training_data(word_list, num_samples, max_word_length)\n",
    "\n",
    "def train_model_parallel(word_list, model_path='large_hangman_model_super_parallel.pth', num_samples=10000, epochs=25, batch_size=32, lr=0.001, num_workers=4):\n",
    "    \"\"\"\n",
    "    Train the HangmanLSTM model using parallel data generation.\n",
    "\n",
    "    Args:\n",
    "        word_list (list): List of words for training.\n",
    "        model_path (str): Path to save the trained model.\n",
    "        num_samples (int): Number of training examples to generate.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        num_workers (int): Number of worker processes for parallel data generation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Generate training data in parallel\n",
    "    print(\"Generating training data in parallel...\")\n",
    "    chunk_size = num_samples // num_workers  # Divide workload across workers\n",
    "    pool_args = [(word_list, chunk_size, 10) for _ in range(num_workers)]  # Prepare arguments for workers\n",
    "\n",
    "    with Pool(num_workers) as pool:  # Create a pool of worker processes\n",
    "        data_chunks = pool.map(parallel_generate_training_data, pool_args)  # Generate data in parallel\n",
    "\n",
    "    # Combine all chunks into a single dataset\n",
    "    data = [item for chunk in data_chunks for item in chunk]\n",
    "    print(f\"Generated {len(data)} training samples.\")\n",
    "\n",
    "    # Prepare data tensors for training\n",
    "    print(\"Preparing data tensors...\")\n",
    "    inputs_word = torch.tensor(np.array([item[0] for item in data]), dtype=torch.float32)\n",
    "    inputs_guessed = torch.tensor(np.array([item[1] for item in data]), dtype=torch.float32)\n",
    "    targets = torch.tensor(np.array([item[2] for item in data]), dtype=torch.long)\n",
    "\n",
    "    # Create a DataLoader to handle batching\n",
    "    print(\"Creating DataLoader...\")\n",
    "    dataset = torch.utils.data.TensorDataset(inputs_word, inputs_guessed, targets)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    model = HangmanLSTM().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training loop...\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):  # Iterate through all epochs\n",
    "        total_loss = 0\n",
    "        for word_batch, guessed_batch, target_batch in dataloader:  # Iterate through all batches\n",
    "            word_batch = word_batch.to(device)\n",
    "            guessed_batch = guessed_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(word_batch, guessed_batch)  # Forward pass\n",
    "            loss = criterion(outputs, target_batch)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)  # Calculate average loss\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")  # Display epoch loss\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the word list from the file\n",
    "    words_file = \"words.txt\"\n",
    "    try:\n",
    "        with open(words_file, 'r') as f:\n",
    "            word_list = [line.strip().lower() for line in f if line.strip()]  # Read and clean words\n",
    "        print(f\"Loaded {len(word_list)} words from {words_file}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {words_file} not found.\")  # Handle file not found error\n",
    "        exit(1)\n",
    "\n",
    "    # Train the model in parallel\n",
    "    print(\"Starting parallel training...\")\n",
    "    start_time = time.time()\n",
    "    train_model_parallel(word_list, model_path=\"trained_models/large_hangman_model_super_parallel.pth\", num_samples=20000, epochs=25, batch_size=32, lr=0.001, num_workers=4)\n",
    "    end_time = time.time()\n",
    "    print(f\"Parallel training completed in {end_time - start_time:.2f} seconds.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMBiIRBjKArE",
    "outputId": "c7d7ed5f-80e7-41f3-84de-3a6fa8e2e2ee"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 852 words from words.txt.\n",
      "Starting parallel training...\n",
      "Using device: cuda\n",
      "Generating training data in parallel...\n",
      "Generated 244091 training samples.\n",
      "Preparing data tensors...\n",
      "Creating DataLoader...\n",
      "Starting training loop...\n",
      "Epoch 1/25, Loss: 2.9479\n",
      "Epoch 2/25, Loss: 2.8439\n",
      "Epoch 3/25, Loss: 2.8109\n",
      "Epoch 4/25, Loss: 2.8061\n",
      "Epoch 5/25, Loss: 2.8026\n",
      "Epoch 6/25, Loss: 2.7988\n",
      "Epoch 7/25, Loss: 2.7963\n",
      "Epoch 8/25, Loss: 2.7958\n",
      "Epoch 9/25, Loss: 2.7946\n",
      "Epoch 10/25, Loss: 2.7910\n",
      "Epoch 11/25, Loss: 2.7888\n",
      "Epoch 12/25, Loss: 2.7840\n",
      "Epoch 13/25, Loss: 2.7802\n",
      "Epoch 14/25, Loss: 2.7763\n",
      "Epoch 15/25, Loss: 2.7718\n",
      "Epoch 16/25, Loss: 2.7674\n",
      "Epoch 17/25, Loss: 2.7634\n",
      "Epoch 18/25, Loss: 2.7574\n",
      "Epoch 19/25, Loss: 2.7520\n",
      "Epoch 20/25, Loss: 2.7473\n",
      "Epoch 21/25, Loss: 2.7417\n",
      "Epoch 22/25, Loss: 2.7378\n",
      "Epoch 23/25, Loss: 2.7330\n",
      "Epoch 24/25, Loss: 2.7285\n",
      "Epoch 25/25, Loss: 2.7239\n",
      "Model saved to large_hangman_model_super_parallel.pth\n",
      "Parallel training completed in 604.43 seconds.\n"
     ]
    }
   ]
  }
 ]
}
